# Introduction

Most methods have been action-value methods, meaning that they learn the values of actions and then select actions based on there estimated action values. This means that there policies would not even exist without the action-value estimates. There can be parameterized policy that can select actions without consulting a value function. The notation $\theta\in R^d$ for the policy paramter vector. 

# Policy Approximation and its Advantages

If the action space is not to discrete and not too large, then a natural and common kind of paramterization is to form a paramterized numerical preferences $h(s,a,\theta)\in R$ for each state action pair. The actions with the highest preferences in each state are given the highest probabilities of being selected, for example, according to an exponential soft-max distribution:

$pi(a|s,\theta)=\frac{e^{h(s,a,\theta)}}{\sum_{b}e^{h(s,b,\theta)}}$ This kind of policy is called soft-max in action preferences. The action preferences themselves can be parameeriozed arbitarily. For example, they might be computed by a deep artificial neural network where $\theta$ is the vector of all the connection weights of the network. Or the preference could be linear in features such as:
$h(s,a,\theta)=\theta^Tx(s,a)$

Advantages of parameterizing policies according to the soft-max in action preferences is that the approximate policy can approach a deterministic policy whereas episilon greedy action selection over action values there is always an epsilon probability of selecting a random action. Second advantage of parameterizing policies according to soft-max is that it enables the selection of actions with arbitrary probabilities. In problems with significant function approximation, the best approximate policy may be stochastic. **Action-value methods have no natural way of finding stochastic optimal policies, where policy approximating methods can**

The choice of policy parameterization is sometimes a good way of injecting prior knowledge about the desired form of the policy into the reinforcement learning system. This is often the most important reason for using a policy-based learning method.

# The Policy Gradient Theorem

With function approximation, it may seem challenging to change the policy parameter in a way that ensures improvement. The problem is that performance depends on both the action selections and the distribution of states in which those selections are made, and that both of these are a↵ected by the policy parameter. Given a state, the eect of the policy parameter on the actions, and thus on reward, can be computed in a relatively straightforward way from knowledge of the parameterization. But the eect of the policy on the state distribution is a function of the environment and is typically unknown. How can we estimate the performance gradient with respect to the policy parameter when the gradient depends on the unknown e↵ect of policy changes on the state distribution? Fortunately, there is an excellent theoretical answer to this challenge in the form of the policy gradient theorem.

$\nabla J(\theta) \propto \sum_{s}\mu(s)\sum_{a}q_{\pi}(s,a)\nabla\pi(a|s,\theta)$

# Actor-Critic Methods
Although the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actor–critic method because its state-value function is used only as a baseline, not as a critic. That is, it is not used for bootstrapping (updating the value estimate for a state from the estimated values of subsequent states), but only as a baseline for the state whose estimate is being updated. This is a useful distinction, for only through bootstrapping do we introduce bias and an asymptotic dependence on the quality of the function approximation. As we have seen, the bias introduced through bootstrapping and reliance on the state representation is often beneficial because it reduces variance and accelerates learning. REINFORCE with baseline is unbiased and will converge asymptotically to a local minimum, but like all Monte Carlo methods it tends to learn slowly (produce estimates of high variance) and to be inconvenient to implement online or for continuing problems. As we have seen earlier in this book, with temporal-di↵erence methods we can eliminate these inconveniences, and through multi-step methods we can flexibly choose the degree of bootstrapping. In order to gain these advantages in the case of policy gradient methods we use actor–critic methods with a bootstrapping critic.